{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34c960a3-9dc3-4583-85f3-433707550778",
   "metadata": {},
   "source": [
    "# 6.2 勾配消失とLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e150ebe-453b-4efe-9efe-a78c0282f0b9",
   "metadata": {},
   "source": [
    "- RNNには勾配消失という問題点あり<br>\n",
    "↓<br>\n",
    "- ゲート付きRNN\n",
    "    - 代表的なモデル\n",
    "        - LSTM ⇦この節で取り上げ\n",
    "        - GTU etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dbc113-8817-4c76-854a-69f395d79e42",
   "metadata": {},
   "source": [
    "## 6.2.1 LSTMのインターフェース"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b5c307-5bc2-470e-bd57-3fb903227570",
   "metadata": {},
   "source": [
    "- RNN記法を下図のように簡略化\n",
    "- $ tanh(h_{t-1}W_h + x_tW_x + b) $ という計算を「tanh」と表す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e78647e-1683-494a-b209-8661cda931cf",
   "metadata": {},
   "source": [
    "![fig6](fig6/6_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd4c2b-ee4b-4ae9-a3fe-d85ed57dd334",
   "metadata": {},
   "source": [
    "- RNNレイヤとLSTMレイヤの比較(下図)\n",
    "    - LSTMにはc(記憶セル)あり\n",
    "        - 記憶セルの特徴\n",
    "            - 自分自身だけで(LSTMレイヤ内だけで)データの受け渡し・他レイヤへの出力はしない"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c72e7e-78bb-4b6d-9f73-34f45c466063",
   "metadata": {},
   "source": [
    "![fig6](fig6/6_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b55e8f-0ed6-4251-93c1-1b1c47281d4e",
   "metadata": {},
   "source": [
    "## 6.2.2 LSTMレイヤの組み立て"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d7620-56a6-4260-abf1-b4b50ffd2045",
   "metadata": {},
   "source": [
    "- 現在の記憶セル$c_t$は($c_{t-1}, h_{t-1}, x_t$)からの計算によって算出\n",
    "- ポイント\n",
    "    - 更新された$c_t$を使って、隠れ状態の$h_t$が計算される\n",
    "        - $h_t = tanh(c_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e9d4d-246c-43a4-9ab1-4ef86ef17aea",
   "metadata": {},
   "source": [
    "![fig6](fig6/6_12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643d2660-2a6f-4fe6-ad9e-a57936b195ed",
   "metadata": {},
   "source": [
    "- LSTMにおける「ゲート」の機能\n",
    "    - どれだけゲートを開くか、水を次へ流すかをコントロール\n",
    "        - 開き具合は、0.0~1.0 までの実数で表される\n",
    "        - ゲートの開き具合のコントロールの為に、専用の重みパラメータが用いられる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8517b8a6-9daa-4f15-97fe-d1b8220dc1db",
   "metadata": {},
   "source": [
    "![6_14](fig6/6_14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a356875-2d39-4d51-9f39-55e29dd1d745",
   "metadata": {},
   "source": [
    "## 6.2.3 outputゲート"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0557a226-c438-485d-85a3-252e8b745441",
   "metadata": {},
   "source": [
    "- outputゲート\n",
    "    - $tanh(c_t)$ の各要素に対して「それらが次時刻の隠れ状態($h_t$)としてどれだけ重要か」を調整\n",
    "    - 開き具合(次へ何%だけ通すか)は、入力$x_t$と前の状態$h_{t-1}$から求める\n",
    "    - ここで使用する重みパラメータやバイアスの上付き文字に$o$(outputの頭文字)を使用\n",
    "    - sigmoind関数はα()で表す\n",
    "    ![6_1](fig6/6_1.png)\n",
    "    - $o$ と $tanh(c_t)$の要素ごとの積を$h_t$として出力\n",
    "    ![6_15](fig6/6_15.png)\n",
    "    - outputゲートで行う式の計算を「α」で表す\n",
    "    - $h_t$は$o$と$tanh(c_t)$の積によって計算\n",
    "        - アダマール積\n",
    "        ![6_2](fig6/6_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30781a82-39ec-4979-a220-1179dad3c40d",
   "metadata": {},
   "source": [
    "## 6.2.4 forgetゲート"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f6a56d-363b-4cc7-a5ab-ee3e0caba7f1",
   "metadata": {},
   "source": [
    "- 記憶セル($c$)に対して「何を忘れるか」を明示的に指示\n",
    "- ![6_16](fig6/6_16.png)\n",
    "- ![6_3](fig6/6_3.png)\n",
    "    - 式(6.3)によってforgetゲートの出力fが求められる\n",
    "    - このfと前の記憶セルである$c_{t-1}$との要素ごとの積($c_t = f⊙c_{t-1}$)によって$c_{t}$が求められる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc8bd65-94c8-428c-9fda-5c71cfcb9e36",
   "metadata": {},
   "source": [
    "## 6.2.5 新しい記憶セル"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05003208-4589-45c2-a3c6-ff432f51be4a",
   "metadata": {},
   "source": [
    "- 新しく覚えるべき情報を記憶セルに追加する必要あり(現状は忘れる機能のみ)\n",
    "- 下図のようにtanhノードを新たに追加\n",
    "![6_17](fig6/6_17.png)\n",
    "- tanhノードによって計算された結果が前時刻の記憶セル$c_{t-1}$に加算される\n",
    "- このノードは「ゲート」ではなく新しい「情報」を記憶セルに追加することが目的\n",
    "![6_4](fig6/6_4.png)\n",
    "- このgが前時刻の$c_{t-1}$に加算されることで新しい記憶が生まれる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36c1743-ea4a-47d2-ac47-de65f547f15f",
   "metadata": {},
   "source": [
    "## 6.2.6 inputゲート"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc91219-0e73-49dd-93c4-ac39139d13e7",
   "metadata": {},
   "source": [
    "- gに対してゲートを加える\n",
    "![6_18](fig6/6_18.png)\n",
    "- gの各要素が新たに追加する情報としてどれだけ価値があるかを判断\n",
    "- inputゲートによって重みづけされた情報が新たに追加される\n",
    "- inputゲートを「α」で表し、その出力をiとする\n",
    "![6_5](fig6/6_5.png)\n",
    "- iとgの要素ごとの積の結果を記憶せるに追加"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955f34de-f81a-46a8-bb3f-26b27a7a620e",
   "metadata": {},
   "source": [
    "## 6.2.7 LSTMの勾配の流れ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e8735e-0510-4c75-9e43-241686407556",
   "metadata": {},
   "source": [
    "- LSTMが勾配消失を起こさない理由\n",
    "    - →記憶セルcの逆伝播に注目すると見えてくる\n",
    "\n",
    "![6_19](fig6/6_19.png)\n",
    "\n",
    "- 記憶セルにフォーカス\n",
    "    - 「+」ノード・・・上流から伝わる勾配をそのまま流すだけ→勾配の変化(劣化)は無し\n",
    "    - 「×」ノード・・・「要素ごとの積(アダマール積)」\n",
    "        - RNNの逆伝播では行列の積を繰り返し行ってきたので勾配消失・爆発が発生\n",
    "        - LSTMの逆伝播では、毎時刻異なるゲート値によって要素ごとの積を計算<br>→勾配消失を起こさない理由\n",
    "        - 「×」ノードの計算はforgetゲートによってコントロール\n",
    "            - 「忘れるべき」と導いた勾配の要素は小さく\n",
    "            - 「忘れてはいけない」と導いた勾配の要素は劣化することなく過去方向へ伝わる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc018cb3-ccef-4540-b32f-83ad520c5048",
   "metadata": {},
   "source": [
    "# 6.3 LSTMの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be8a9d-d20b-43c0-8d72-9d597f3858a5",
   "metadata": {},
   "source": [
    "- 最初に、1ステップの処理をLSTMクラスとして実装\n",
    "- そして、Tステップ分をまとめて処理するクラスをTimeLSTMとして実装\n",
    "-　LSTMで行う計算は下図 <br>\n",
    "![6_6](fig6/6_6.png)<br>\n",
    "- 式(6.6)の4つのアフィン変換※がポイント(※ $xW_x + hW_h + b$のような式)\n",
    "        - 一つの式にまとめて計算可能\n",
    "![6_20](fig6/6_20.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef51bdea-a504-4f37-9f6e-eba4e0420995",
   "metadata": {},
   "source": [
    "- 本来であれば4回個別に行っていたアフィン変換の計算を1回の計算で済ませることが可能に<br>→計算の高速化\n",
    "- $W_x, W_h, b$　にそれぞれ4つ分の重みが含まれていると仮定して、LSTMの計算グラフを図示<br>\n",
    "![6_21](fig6/6_21.png)\n",
    "- ここでは初めに4つ分のアフィン変換をまとめて実施\n",
    "-　そして、sliceノードによって、その4つの結果を取り出す\n",
    "    - sliceノード・・・アフィン変換の結果を4分割して取り出すだけの単純なノード"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1007c86d-5bea-4a7c-aaa8-bbbadfeff132",
   "metadata": {},
   "source": [
    "◆LSTMクラスの初期化コード\n",
    "```python\n",
    "class LSTM:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        '''\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Wx: 入力`x`用の重みパラーメタ（4つ分の重みをまとめる）\n",
    "        Wh: 隠れ状態`h`用の重みパラメータ（4つ分の重みをまとめる）\n",
    "        b: バイアス（4つ分のバイアスをまとめる）\n",
    "        '''\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "```\n",
    "\n",
    "- 初期化の引数は、重みパラメータのWx, Wh, b\n",
    "    - 重みには4つ分の重みが纏められている\n",
    "    - パラメータはメンバ変数のparamsに設定・初期化\n",
    "    - cacheは順伝播での中間結果を保持するために使用。逆伝播の計算でも使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a479b92-d9eb-41c6-8447-f4da929f2f86",
   "metadata": {},
   "source": [
    "◆順伝播の実装<br>\n",
    "- 引数は、現時刻の入力x、前時刻の隠れ状態h_prev、前時刻の記憶セルc_prev\n",
    "\n",
    "```python\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, H = h_prev.shape\n",
    "\n",
    "        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n",
    "\n",
    "        f = A[:, :H]\n",
    "        g = A[:, H:2*H]\n",
    "        i = A[:, 2*H:3*H]\n",
    "        o = A[:, 3*H:]\n",
    "\n",
    "        f = sigmoid(f)\n",
    "        g = np.tanh(g)\n",
    "        i = sigmoid(i)\n",
    "        o = sigmoid(o)\n",
    "\n",
    "        c_next = f * c_prev + g * i\n",
    "        h_next = o * np.tanh(c_next)\n",
    "\n",
    "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n",
    "        return h_next, c_next\n",
    "```\n",
    "- まず初めにアフィン変換が行われる\n",
    " - メンバ変数のWx、Wh、bには4つ分のパラメータが格納。形状は以下の通り\n",
    " ![6_22](fig6/6_22.png)\n",
    "- バッチ数をN、入力データの次元数をD、記憶セルと隠れ状態の次元数を両者ともH\n",
    "- 計算結果のA には4 つ分のアフィン変換の結果が格納\n",
    "- そこからA[:, :H] やA[:, H:2*H] のようにスライスして取り出すことで、それ以降の演算ノードへ分配"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382e20d-c394-4f69-b299-bb04f1a84cfc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dce10ba4-e3f8-4f0d-8b85-7f6dfa0ca23a",
   "metadata": {},
   "source": [
    "◆LSTMの逆伝播\n",
    "- 逆伝播では、4つの勾配を結合する必要あり\n",
    " ![6_23](fig6/6_23.png)\n",
    " \n",
    " \n",
    "```python\n",
    "    def backward(self, dh_next, dc_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n",
    "\n",
    "        tanh_c_next = np.tanh(c_next)\n",
    "\n",
    "        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
    "\n",
    "        dc_prev = ds * f\n",
    "\n",
    "        di = ds * g\n",
    "        df = ds * c_prev\n",
    "        do = dh_next * tanh_c_next\n",
    "        dg = ds * i\n",
    "\n",
    "        di *= i * (1 - i)\n",
    "        df *= f * (1 - f)\n",
    "        do *= o * (1 - o)\n",
    "        dg *= (1 - g ** 2)\n",
    "\n",
    "        dA = np.hstack((df, dg, di, do))\n",
    "\n",
    "        dWh = np.dot(h_prev.T, dA)\n",
    "        dWx = np.dot(x.T, dA)\n",
    "        db = dA.sum(axis=0)\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        dx = np.dot(dA, Wx.T)\n",
    "        dh_prev = np.dot(dA, Wh.T)\n",
    "\n",
    "        return dx, dh_prev, dc_prev\n",
    "```\n",
    "- 4つの勾配(df, dg, di, do)を連結してdAを作成\n",
    "    - np.hstack()　が使用可能<br>引数に与えられた配列を横方向に連結"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6b03ec-e20c-4206-a19b-7cb9758aa7b7",
   "metadata": {},
   "source": [
    "## 6.3.1 TimeLSTMの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e767f5d9-1225-422f-baae-22359ff1005d",
   "metadata": {},
   "source": [
    "- T個分の時系列データをまとめて処理するレイヤ\n",
    "![6_24](fig6/6_24.png)\n",
    "- RNNで学習を行う際には、Truncated BPTTを行う\n",
    "    - 逆伝播のつながりを適当な長さで断ち切る\n",
    "![6_25](fig6/6_25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e078c590-4e0c-499c-923b-700c80bfbdbb",
   "metadata": {},
   "source": [
    "◆Time LSTMの実装\n",
    "- LSTM では隠れ状態のh に加えて記憶セルc も用いますが、TimeLSTM クラスの実装はTimeRNN クラスの場合とほとんど同じです。ここでも引数のstateful によって状態を維持するかどうかを指定します。\n",
    "\n",
    "```python\n",
    "class TimeLSTM:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.c = None, None\n",
    "        self.dh = None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        H = Wh.shape[0]\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "        if not self.stateful or self.c is None:\n",
    "            self.c = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = LSTM(*self.params)\n",
    "            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\n",
    "            hs[:, t, :] = self.h\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D = Wx.shape[0]\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh, dc = 0, 0\n",
    "\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\n",
    "            dxs[:, t, :] = dx\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h, c=None):\n",
    "        self.h, self.c = h, c\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h, self.c = None, None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a03f92-acfe-405b-b6aa-3d263c475baf",
   "metadata": {},
   "source": [
    "# 6.4 LSTMを使った言語モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486e05d-fc9b-47a1-ade9-bae443c10ad4",
   "metadata": {},
   "source": [
    "\n",
    "![6_26](fig6/6_26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda254ef-e632-44f4-9d36-f39ddfbf1a0e",
   "metadata": {},
   "source": [
    "- Rnnlmというクラスで実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa56a8c3-be20-40bd-8f12-459635b7cc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "from common.base_model import BaseModel\n",
    "\n",
    "\n",
    "class Rnnlm(BaseModel):\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        # 重みの初期化\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layer = self.layers[1]\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        score = self.predict(xs)\n",
    "        loss = self.loss_layer.forward(score, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.lstm_layer.reset_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af536e6b-ceb8-47f4-8f49-b298482edc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 1327 | time 0[s] | perplexity 10000.47\n",
      "| epoch 1 |  iter 21 / 1327 | time 4[s] | perplexity 3106.71\n",
      "| epoch 1 |  iter 41 / 1327 | time 8[s] | perplexity 1241.52\n",
      "| epoch 1 |  iter 61 / 1327 | time 12[s] | perplexity 981.25\n",
      "| epoch 1 |  iter 81 / 1327 | time 16[s] | perplexity 805.74\n",
      "| epoch 1 |  iter 101 / 1327 | time 20[s] | perplexity 654.71\n",
      "| epoch 1 |  iter 121 / 1327 | time 24[s] | perplexity 659.23\n",
      "| epoch 1 |  iter 141 / 1327 | time 29[s] | perplexity 606.01\n",
      "| epoch 1 |  iter 161 / 1327 | time 33[s] | perplexity 577.84\n",
      "| epoch 1 |  iter 181 / 1327 | time 37[s] | perplexity 573.54\n",
      "| epoch 1 |  iter 201 / 1327 | time 41[s] | perplexity 496.37\n",
      "| epoch 1 |  iter 221 / 1327 | time 46[s] | perplexity 485.98\n",
      "| epoch 1 |  iter 241 / 1327 | time 50[s] | perplexity 442.10\n",
      "| epoch 1 |  iter 261 / 1327 | time 55[s] | perplexity 463.54\n",
      "| epoch 1 |  iter 281 / 1327 | time 59[s] | perplexity 450.37\n",
      "| epoch 1 |  iter 301 / 1327 | time 64[s] | perplexity 390.10\n",
      "| epoch 1 |  iter 321 / 1327 | time 69[s] | perplexity 347.13\n",
      "| epoch 1 |  iter 341 / 1327 | time 73[s] | perplexity 402.78\n",
      "| epoch 1 |  iter 361 / 1327 | time 77[s] | perplexity 403.72\n",
      "| epoch 1 |  iter 381 / 1327 | time 81[s] | perplexity 334.52\n",
      "| epoch 1 |  iter 401 / 1327 | time 86[s] | perplexity 346.43\n",
      "| epoch 1 |  iter 421 / 1327 | time 90[s] | perplexity 339.43\n",
      "| epoch 1 |  iter 441 / 1327 | time 94[s] | perplexity 329.94\n",
      "| epoch 1 |  iter 461 / 1327 | time 98[s] | perplexity 325.94\n",
      "| epoch 1 |  iter 481 / 1327 | time 102[s] | perplexity 298.99\n",
      "| epoch 1 |  iter 501 / 1327 | time 106[s] | perplexity 304.61\n",
      "| epoch 1 |  iter 521 / 1327 | time 110[s] | perplexity 299.10\n",
      "| epoch 1 |  iter 541 / 1327 | time 115[s] | perplexity 314.85\n",
      "| epoch 1 |  iter 561 / 1327 | time 119[s] | perplexity 286.98\n",
      "| epoch 1 |  iter 581 / 1327 | time 123[s] | perplexity 259.07\n",
      "| epoch 1 |  iter 601 / 1327 | time 127[s] | perplexity 336.67\n",
      "| epoch 1 |  iter 621 / 1327 | time 132[s] | perplexity 309.83\n",
      "| epoch 1 |  iter 641 / 1327 | time 136[s] | perplexity 281.48\n",
      "| epoch 1 |  iter 661 / 1327 | time 140[s] | perplexity 267.94\n",
      "| epoch 1 |  iter 681 / 1327 | time 145[s] | perplexity 228.29\n",
      "| epoch 1 |  iter 701 / 1327 | time 149[s] | perplexity 251.14\n",
      "| epoch 1 |  iter 721 / 1327 | time 154[s] | perplexity 260.00\n",
      "| epoch 1 |  iter 741 / 1327 | time 158[s] | perplexity 223.57\n",
      "| epoch 1 |  iter 761 / 1327 | time 162[s] | perplexity 235.45\n",
      "| epoch 1 |  iter 781 / 1327 | time 167[s] | perplexity 222.24\n",
      "| epoch 1 |  iter 801 / 1327 | time 171[s] | perplexity 241.94\n",
      "| epoch 1 |  iter 821 / 1327 | time 175[s] | perplexity 228.70\n",
      "| epoch 1 |  iter 841 / 1327 | time 179[s] | perplexity 227.97\n",
      "| epoch 1 |  iter 861 / 1327 | time 184[s] | perplexity 224.24\n",
      "| epoch 1 |  iter 881 / 1327 | time 188[s] | perplexity 207.62\n",
      "| epoch 1 |  iter 901 / 1327 | time 193[s] | perplexity 253.84\n",
      "| epoch 1 |  iter 921 / 1327 | time 197[s] | perplexity 225.62\n",
      "| epoch 1 |  iter 941 / 1327 | time 201[s] | perplexity 230.88\n",
      "| epoch 1 |  iter 961 / 1327 | time 206[s] | perplexity 247.27\n",
      "| epoch 1 |  iter 981 / 1327 | time 210[s] | perplexity 229.67\n",
      "| epoch 1 |  iter 1001 / 1327 | time 214[s] | perplexity 194.80\n",
      "| epoch 1 |  iter 1021 / 1327 | time 218[s] | perplexity 226.93\n",
      "| epoch 1 |  iter 1041 / 1327 | time 223[s] | perplexity 213.19\n",
      "| epoch 1 |  iter 1061 / 1327 | time 227[s] | perplexity 197.80\n",
      "| epoch 1 |  iter 1081 / 1327 | time 231[s] | perplexity 170.81\n",
      "| epoch 1 |  iter 1101 / 1327 | time 236[s] | perplexity 192.62\n",
      "| epoch 1 |  iter 1121 / 1327 | time 240[s] | perplexity 230.38\n",
      "| epoch 1 |  iter 1141 / 1327 | time 244[s] | perplexity 208.91\n",
      "| epoch 1 |  iter 1161 / 1327 | time 249[s] | perplexity 200.69\n",
      "| epoch 1 |  iter 1181 / 1327 | time 253[s] | perplexity 191.55\n",
      "| epoch 1 |  iter 1201 / 1327 | time 258[s] | perplexity 164.46\n",
      "| epoch 1 |  iter 1221 / 1327 | time 262[s] | perplexity 161.37\n",
      "| epoch 1 |  iter 1241 / 1327 | time 266[s] | perplexity 189.35\n",
      "| epoch 1 |  iter 1261 / 1327 | time 271[s] | perplexity 173.76\n",
      "| epoch 1 |  iter 1281 / 1327 | time 275[s] | perplexity 180.33\n",
      "| epoch 1 |  iter 1301 / 1327 | time 280[s] | perplexity 224.38\n",
      "| epoch 1 |  iter 1321 / 1327 | time 285[s] | perplexity 211.78\n",
      "| epoch 2 |  iter 1 / 1327 | time 286[s] | perplexity 223.98\n",
      "| epoch 2 |  iter 21 / 1327 | time 291[s] | perplexity 205.61\n",
      "| epoch 2 |  iter 41 / 1327 | time 296[s] | perplexity 191.58\n",
      "| epoch 2 |  iter 61 / 1327 | time 300[s] | perplexity 177.84\n",
      "| epoch 2 |  iter 81 / 1327 | time 304[s] | perplexity 159.57\n",
      "| epoch 2 |  iter 101 / 1327 | time 309[s] | perplexity 155.49\n",
      "| epoch 2 |  iter 121 / 1327 | time 313[s] | perplexity 161.77\n",
      "| epoch 2 |  iter 141 / 1327 | time 318[s] | perplexity 178.33\n",
      "| epoch 2 |  iter 161 / 1327 | time 322[s] | perplexity 194.24\n",
      "| epoch 2 |  iter 181 / 1327 | time 326[s] | perplexity 202.42\n",
      "| epoch 2 |  iter 201 / 1327 | time 331[s] | perplexity 188.49\n",
      "| epoch 2 |  iter 221 / 1327 | time 335[s] | perplexity 184.40\n",
      "| epoch 2 |  iter 241 / 1327 | time 340[s] | perplexity 177.20\n",
      "| epoch 2 |  iter 261 / 1327 | time 345[s] | perplexity 187.47\n",
      "| epoch 2 |  iter 281 / 1327 | time 349[s] | perplexity 187.45\n",
      "| epoch 2 |  iter 301 / 1327 | time 354[s] | perplexity 168.74\n",
      "| epoch 2 |  iter 321 / 1327 | time 358[s] | perplexity 139.80\n",
      "| epoch 2 |  iter 341 / 1327 | time 363[s] | perplexity 174.70\n",
      "| epoch 2 |  iter 361 / 1327 | time 367[s] | perplexity 198.88\n",
      "| epoch 2 |  iter 381 / 1327 | time 371[s] | perplexity 156.33\n",
      "| epoch 2 |  iter 401 / 1327 | time 376[s] | perplexity 168.81\n",
      "| epoch 2 |  iter 421 / 1327 | time 380[s] | perplexity 158.07\n",
      "| epoch 2 |  iter 441 / 1327 | time 385[s] | perplexity 165.06\n",
      "| epoch 2 |  iter 461 / 1327 | time 390[s] | perplexity 159.37\n",
      "| epoch 2 |  iter 481 / 1327 | time 395[s] | perplexity 158.33\n",
      "| epoch 2 |  iter 501 / 1327 | time 400[s] | perplexity 171.95\n",
      "| epoch 2 |  iter 521 / 1327 | time 404[s] | perplexity 174.34\n",
      "| epoch 2 |  iter 541 / 1327 | time 409[s] | perplexity 176.70\n",
      "| epoch 2 |  iter 561 / 1327 | time 414[s] | perplexity 157.03\n",
      "| epoch 2 |  iter 581 / 1327 | time 419[s] | perplexity 140.83\n",
      "| epoch 2 |  iter 601 / 1327 | time 423[s] | perplexity 192.08\n",
      "| epoch 2 |  iter 621 / 1327 | time 428[s] | perplexity 185.89\n",
      "| epoch 2 |  iter 641 / 1327 | time 433[s] | perplexity 164.86\n",
      "| epoch 2 |  iter 661 / 1327 | time 437[s] | perplexity 155.17\n",
      "| epoch 2 |  iter 681 / 1327 | time 442[s] | perplexity 131.27\n",
      "| epoch 2 |  iter 701 / 1327 | time 446[s] | perplexity 153.50\n",
      "| epoch 2 |  iter 721 / 1327 | time 451[s] | perplexity 162.51\n",
      "| epoch 2 |  iter 741 / 1327 | time 455[s] | perplexity 135.99\n",
      "| epoch 2 |  iter 761 / 1327 | time 460[s] | perplexity 135.17\n",
      "| epoch 2 |  iter 781 / 1327 | time 464[s] | perplexity 136.14\n",
      "| epoch 2 |  iter 801 / 1327 | time 469[s] | perplexity 151.09\n",
      "| epoch 2 |  iter 821 / 1327 | time 474[s] | perplexity 146.58\n",
      "| epoch 2 |  iter 841 / 1327 | time 479[s] | perplexity 145.76\n",
      "| epoch 2 |  iter 861 / 1327 | time 484[s] | perplexity 149.11\n",
      "| epoch 2 |  iter 881 / 1327 | time 488[s] | perplexity 131.55\n",
      "| epoch 2 |  iter 901 / 1327 | time 493[s] | perplexity 167.34\n",
      "| epoch 2 |  iter 921 / 1327 | time 497[s] | perplexity 147.36\n",
      "| epoch 2 |  iter 941 / 1327 | time 502[s] | perplexity 155.28\n",
      "| epoch 2 |  iter 961 / 1327 | time 506[s] | perplexity 167.40\n",
      "| epoch 2 |  iter 981 / 1327 | time 511[s] | perplexity 156.53\n",
      "| epoch 2 |  iter 1001 / 1327 | time 515[s] | perplexity 132.96\n",
      "| epoch 2 |  iter 1021 / 1327 | time 519[s] | perplexity 158.79\n",
      "| epoch 2 |  iter 1041 / 1327 | time 524[s] | perplexity 145.45\n",
      "| epoch 2 |  iter 1061 / 1327 | time 528[s] | perplexity 130.57\n",
      "| epoch 2 |  iter 1081 / 1327 | time 533[s] | perplexity 113.42\n",
      "| epoch 2 |  iter 1101 / 1327 | time 538[s] | perplexity 123.16\n",
      "| epoch 2 |  iter 1121 / 1327 | time 543[s] | perplexity 156.78\n",
      "| epoch 2 |  iter 1141 / 1327 | time 547[s] | perplexity 144.80\n",
      "| epoch 2 |  iter 1161 / 1327 | time 552[s] | perplexity 134.18\n",
      "| epoch 2 |  iter 1181 / 1327 | time 557[s] | perplexity 135.53\n",
      "| epoch 2 |  iter 1201 / 1327 | time 561[s] | perplexity 114.37\n",
      "| epoch 2 |  iter 1221 / 1327 | time 566[s] | perplexity 111.10\n",
      "| epoch 2 |  iter 1241 / 1327 | time 570[s] | perplexity 133.14\n",
      "| epoch 2 |  iter 1261 / 1327 | time 575[s] | perplexity 124.94\n",
      "| epoch 2 |  iter 1281 / 1327 | time 579[s] | perplexity 124.84\n",
      "| epoch 2 |  iter 1301 / 1327 | time 584[s] | perplexity 160.40\n",
      "| epoch 2 |  iter 1321 / 1327 | time 588[s] | perplexity 155.40\n",
      "| epoch 3 |  iter 1 / 1327 | time 590[s] | perplexity 161.89\n",
      "| epoch 3 |  iter 21 / 1327 | time 594[s] | perplexity 145.78\n",
      "| epoch 3 |  iter 41 / 1327 | time 599[s] | perplexity 136.24\n",
      "| epoch 3 |  iter 61 / 1327 | time 603[s] | perplexity 130.29\n",
      "| epoch 3 |  iter 81 / 1327 | time 608[s] | perplexity 118.74\n",
      "| epoch 3 |  iter 101 / 1327 | time 612[s] | perplexity 107.63\n",
      "| epoch 3 |  iter 121 / 1327 | time 617[s] | perplexity 118.38\n",
      "| epoch 3 |  iter 141 / 1327 | time 621[s] | perplexity 127.73\n",
      "| epoch 3 |  iter 161 / 1327 | time 626[s] | perplexity 144.00\n",
      "| epoch 3 |  iter 181 / 1327 | time 630[s] | perplexity 152.57\n",
      "| epoch 3 |  iter 201 / 1327 | time 635[s] | perplexity 143.29\n",
      "| epoch 3 |  iter 221 / 1327 | time 639[s] | perplexity 142.61\n",
      "| epoch 3 |  iter 241 / 1327 | time 644[s] | perplexity 135.14\n",
      "| epoch 3 |  iter 261 / 1327 | time 649[s] | perplexity 142.52\n",
      "| epoch 3 |  iter 281 / 1327 | time 654[s] | perplexity 144.83\n",
      "| epoch 3 |  iter 301 / 1327 | time 658[s] | perplexity 127.03\n",
      "| epoch 3 |  iter 321 / 1327 | time 663[s] | perplexity 103.76\n",
      "| epoch 3 |  iter 341 / 1327 | time 668[s] | perplexity 126.12\n",
      "| epoch 3 |  iter 361 / 1327 | time 672[s] | perplexity 153.94\n",
      "| epoch 3 |  iter 381 / 1327 | time 677[s] | perplexity 116.80\n",
      "| epoch 3 |  iter 401 / 1327 | time 682[s] | perplexity 130.84\n",
      "| epoch 3 |  iter 421 / 1327 | time 687[s] | perplexity 115.90\n",
      "| epoch 3 |  iter 441 / 1327 | time 692[s] | perplexity 125.89\n",
      "| epoch 3 |  iter 461 / 1327 | time 697[s] | perplexity 121.28\n",
      "| epoch 3 |  iter 481 / 1327 | time 702[s] | perplexity 120.90\n",
      "| epoch 3 |  iter 501 / 1327 | time 707[s] | perplexity 130.11\n",
      "| epoch 3 |  iter 521 / 1327 | time 712[s] | perplexity 139.87\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.optimizer import SGD\n",
    "from common.trainer import RnnlmTrainer\n",
    "from common.util import eval_perplexity\n",
    "from dataset import ptb\n",
    "from rnnlm import Rnnlm\n",
    "\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "batch_size = 20\n",
    "wordvec_size = 100\n",
    "hidden_size = 100  # RNNの隠れ状態ベクトルの要素数\n",
    "time_size = 35  # RNNを展開するサイズ\n",
    "lr = 20.0\n",
    "max_epoch = 4\n",
    "max_grad = 0.25\n",
    "\n",
    "# 学習データの読み込み\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_test, _, _ = ptb.load_data('test')\n",
    "vocab_size = len(word_to_id)\n",
    "xs = corpus[:-1]\n",
    "ts = corpus[1:]\n",
    "\n",
    "# モデルの生成\n",
    "model = Rnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD(lr)\n",
    "trainer = RnnlmTrainer(model, optimizer)\n",
    "\n",
    "# 勾配クリッピングを適用して学習\n",
    "trainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad,\n",
    "            eval_interval=20)\n",
    "trainer.plot(ylim=(0, 500))\n",
    "\n",
    "# テストデータで評価\n",
    "model.reset_state()\n",
    "ppl_test = eval_perplexity(model, corpus_test)\n",
    "print('test perplexity: ', ppl_test)\n",
    "\n",
    "# パラメータの保存\n",
    "model.save_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf95acc-f086-4795-8cb5-3874d47e6118",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84567ff8-8a35-4769-acc7-8e970e3912ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f57848-f655-4109-ad90-e6b152f1f9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4819e49-83b5-4f42-8024-54d7393e758a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
