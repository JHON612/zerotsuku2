{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 カウントベースの手法の改善"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.1 相互情報量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前節の共起行列の要素は、2 つの単語が共起した回数を表しています。しかし、こ\n",
    "の“生” の回数というのはあまり良い性質を持ちません。その理由は、高頻度単語（多\n",
    "く出現する単語）に目を向けるとはっきりします。\n",
    "たとえば、あるコーパスにおいて「the」と「car」の共起を考えてみましょう。そ\n",
    "の場合、「... the car ...」というフレーズは多く見られるでしょう。そのため、その\n",
    "共起する回数は大きな値になります。一方、「car」と「drive」という単語には明ら\n",
    "かに強い関連性があります。しかし、単に出現回数だけを見てしまうと、「car」は「drive」よりも「the」のほうに強い関連性を持ってしまうでしょう。つまりこれは、\n",
    "「the」という単語が高頻度な単語であるがゆえに、「car」と強い関連性を持つように\n",
    "評価されてしまうということです。\n",
    "そのような問題を解決するために、相互情報量（Pointwise Mutual Information）\n",
    "[19] と呼ばれる指標が使われます（以降、PMI と略記）。これはx とy という確率変\n",
    "数に対して次の式で定義されます（確率については「3.5.1 CBOWモデルと確率」で\n",
    "詳しく説明します）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "PMI(x, y) = log_2\\frac{P(x,y)}{P(x)P(y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式(2.2) がPMI の定義式です。ここでP(x) はx が起こる確率、P(y) はy が起\n",
    "こる確率を表します。そして、P(x, y) はx とy が同時に起こる確率を表します。こ\n",
    "のPMI は、その値が高いほど関連性が高いことを示します。\n",
    "これを私たちの自然言語の例に当てはめると、P(x) というのはx という単語がコー\n",
    "パスに現れる確率を指します。たとえば、10,000 個の単語からなるコーパスで「the」\n",
    "という単語が100 回出現したとしましょう。そうすると、$ P(\"the\") = \\frac{100}{10000} = 0.01 $\n",
    "となります。また、P(x, y) は単語x とy が共起する確率を表します。これも例を出\n",
    "すと、たとえば「the」と「car」が10 回共起した場合、$ P(\"the\", \"car\") = \\frac{10}{10000} = 0.001 $\n",
    "となります。\n",
    "それでは共起行列（各要素は共起した単語の回数）を使って、式(2.2) を書き換え\n",
    "てみましょう。ここでは共起行列をC として、単語x とy の共起する回数をC(x, y)\n",
    "で表します。また、単語x、y の出現する回数はそれぞれC(x)、C(y) で表します。\n",
    "このとき、コーパスに含まれる単語数をN とすると、式(2.2) は次のように書き換\n",
    "えられます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "PMI(x, y) = log_2\\frac{P(x,y)}{P(x)P(y)} = log_2\\frac{\\frac{C(x,y)}{N}}{\\frac{C(x)}{N}\\frac{C(y)}{N}} =log_2\\frac{C(x,y)*N}{C(x)C(y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式(2.3) によって、共起行列からPMI を求めることができます。それでは式(2.3)\n",
    "を使って、具体的に計算してみましょう。ここでは、コーパス中の単語数（N）を\n",
    "10,000 として、「the」が1,000 回、「car」が20 回、「drive」が10 回出現したとし\n",
    "ます。そして、「the」と「car」の共起が10 回、「car」と「drive」の共起が5 回だ\n",
    "と仮定しましょう。このとき、共起する回数の視点では、「car」は、「drive」よりも\n",
    "「the」のほうに関連性が強いことになります。では、PMI の視点ではどうでしょうか。これは次のように計算できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "PMI(\"the\", \"car\") = log_2\\frac{10*10000}{1000*20} ≒ 2.32\n",
    "$$\n",
    "$$\n",
    "PMI(\"car\", \"drive\") = log_2\\frac{5*10000}{20*10} ≒ 7.97\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この結果が示すとおり、PMI を用いることで、「car」は、「the」よりも「drive」の\n",
    "ほうに関連性を持つようになりました。これは私たちの要望どおりの結果です。この\n",
    "ような結果になったのは、単語単独の出現回数が考慮されたことによります。この例\n",
    "では、「the」が多く出現していることから、PMI のスコアが低減されたのです。な\n",
    "お、式中の≈ は「ニアリー・イコール」という記号で、近似的に等しいことを意味し\n",
    "ます。\n",
    "これでPMI という良い指標を手にできましたが、このPMI にはひとつ問題があ\n",
    "ります。それは2 つの単語で共起する回数が0 の場合、log2 0 = −∞ となってしま\n",
    "う点です。それに対応するため、実践上では次の正の相互情報量（Positive PMI）が\n",
    "使われます（以降、PPMI と略記）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "PPMI(x, y) = max(0, PMI(x, y))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式(2.6) により、PMI がマイナスのときは、それを0 として扱います。これ\n",
    "で、単語間の関連度を0 以上の実数によって表すことができます。それでは、共\n",
    "起行列をPPMI 行列に変換する関数を実装しましょう。私たちはそれをppmi(C,\n",
    "verbose=False, eps=1e-8) という名前で実装します（ common/util.py）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def ppmi(C, verbose=False, eps = 1e-8):\n",
    "    '''PPMI（正の相互情報量）の作成\n",
    "\n",
    "    :param C: 共起行列\n",
    "    :param verbose: 進行状況を出力するかどうか\n",
    "    :return:\n",
    "    '''\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis=0)\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
    "            M[i, j] = max(0, pmi)\n",
    "\n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total//100 + 1) == 0:\n",
    "                    print('%.1f%% done' % (100*cnt/total))\n",
    "    return M\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで引数のC は共起行列、verbose は進行状況を出力するかどうかを決めるフ\n",
    "ラグを表します。このverbose は大きなコーパスを扱う際に、verbose=True とす\n",
    "ることで、進行状況を確認する用途に利用します。このコードでは、共起行列だけからPPMI 行列を求められるようにするために簡易的な実装を行っています。具体的には、単語x とy の共起する回数をC(x, y) としたとき、\n",
    "$ C(x)=\\sum_{i}C(i,x)$、\n",
    "$ C(y)=\\sum_{i}C(i,y)$、\n",
    "$ N=\\sum_{i}\\sum_{j}C(i,j)$\n",
    "であるものとして――そのような近似を\n",
    "行い――実装しています。また、上のコードではnp.log2(0)=-inf を避けるため\n",
    "にeps という微小な値を使用しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、共起行列をPPMI 行列に変換してみましょう。これは次のように実装\n",
    "できます（ ch02/ppmi.py）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance matrix\n",
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n",
      "--------------------------------------------------\n",
      "PPMI\n",
      "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.807 0.    0.    0.    0.    2.807]\n",
      " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.util import preprocess, create_co_matrix, cos_similarity, ppmi\n",
    "\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "W = ppmi(C)\n",
    "\n",
    "np.set_printoptions(precision=3)  # 有効桁３桁で表示\n",
    "print('covariance matrix')\n",
    "print(C)\n",
    "print('-'*50)\n",
    "print('PPMI')\n",
    "print(W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、共起行列をPPMI 行列に変換することができました。このとき、PPMI\n",
    "行列の各要素は0 以上の実数値です。これで私たちはより良い指標からなる行列――\n",
    "より良い単語ベクトル――を手にできたのです。\n",
    "しかし、このPPMI 行列にも、まだ大きな問題があります。それは、コーパスの\n",
    "語彙数が増えるにつれて、各単語のベクトルの次元数も増えていくという問題です。\n",
    "たとえば、コーパスに含まれる語彙数が10 万に達すれば、そのベクトルの次元数も\n",
    "同様に10 万になります。実際のところ、10 万次元のベクトルを扱うというのはあま\n",
    "り現実的ではありません。\n",
    "また、この行列の中身を見てみると、その要素の多くが0 であることが分かりま\n",
    "す。これは、ベクトルのほとんどの要素が重要ではない――つまり、各要素の持つ\n",
    "“重要度” は低いということを意味します。そして、そのようなベクトルは、ノイズに\n",
    "弱く、頑健性に乏しいという欠点があります。こういった問題に対してよく行われる\n",
    "のが、ベクトルの次元削減です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.2 次元削減"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次元削減（dimensionality reduction）は、文字どおり、ベクトルの次元を削減す\n",
    "る手法を指します。ただし、単に削減するのではなく、“重要な情報” をできるだけ残\n",
    "した上で削減するというところがポイントです。直感的なイメージとしては、図2-8\n",
    "に示すように、データの分布を見て重要な“軸” を見つけることを行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2_8](fig/2_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図2-8 では、元々は2 次元の座標で表されていたデータ点を、データの広がりを考慮して、ひとつの座標軸で表すように新たな軸を導入します。このとき各データ点の値は、新しい軸への射影された値によって表されます。ここで大切な点は、データの広がりを考慮した軸をとることで、1 次元の値だけでもデータの本質的な差異を捉えられるということです。これと同じようなことは、さらに多次元のデータでも行えます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次元削減を行う方法はいくつかあります。ここでは特異値分解（Singular Value\n",
    "Decomposition：SVD）を使った次元削減を行います。SVD は、任意の行列を3 つ\n",
    "の行列の積へと分解します。数式で書くと、次のように表されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ X = USV^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式(2.7) が示すように、SVD は、任意の行列X を、U、S、V の3 つの行列の積\n",
    "に分解します。ここでU とV は直交行列であり、その列ベクトルは互いに直交しま\n",
    "す。また、S は対角行列であり、これは対角成分以外はすべて0 の行列です。このとき、これらの行列を視覚的に表すと、図2-9 のようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2_9](fig/2_9.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて、式(2.7) においてU は直交行列です。そして、この直交行列は何らかの空\n",
    "間の軸（基底）を形成しています。私たちの文脈においては、このU という行列を\n",
    "「単語空間」として扱うことができるのです。また、S は対角行列で、この対角成分\n",
    "には、「特異値」というものが大きい順に並んでいます。特異値とは、簡単に言えば、\n",
    "「対応する軸」の重要度とみなすことができます。そこで、図2-10 のように、重要で\n",
    "ない要素を削ることが考えられます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2_10](fig/2_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図2-10 に示すように、行列S の特異値が小さいものは重要度が低いので、行列U\n",
    "から余分な列ベクトルを削ることで、元の行列を近似することができます。これを私\n",
    "たちが扱っている「単語のPPMI 行列」に当てはめると、行列X の各行には対応す\n",
    "る単語ID の単語ベクトルが格納されており、それらの単語ベクトルが行列U′ とし\n",
    "て次元削減されたベクトルで表現されることになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.3 SVDによる次元削減"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python で実際にSVD を行ってみましょう。このSVD はNumPy のlinalg モ\n",
    "ジュールにあるsvd というメソッドで実行できます。ちなみに、linalg とはlinear\n",
    "algebra（線形代数）の略称です。それでは、共起行列を作り、PPMI 行列に変換し、\n",
    "それに対してSVD を適用します（ ch02/count_method_small.py）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0]\n",
      "[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      "[ 3.409e-01 -1.110e-16 -1.205e-01 -4.163e-16 -9.323e-01 -1.110e-16\n",
      " -2.426e-17]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from common.util import preprocess, create_co_matrix, ppmi\n",
    "\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(id_to_word)\n",
    "C = create_co_matrix(corpus, vocab_size, window_size=1)\n",
    "W = ppmi(C)\n",
    "\n",
    "# SVD\n",
    "U, S, V = np.linalg.svd(W)\n",
    "\n",
    "np.set_printoptions(precision=3)  # 有効桁３桁で表示\n",
    "print(C[0])\n",
    "print(W[0])\n",
    "print(U[0])\n",
    "\n",
    "# plot\n",
    "for word, word_id in word_to_id.items():\n",
    "    plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\n",
    "plt.scatter(U[:,0], U[:,1], alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このプロット図を見ると、「goodbye」と「hello」、「you」と「i」が近い場所に位\n",
    "置することが分かります。これは私たちの直感と比較的近いものでしょう。しかし、\n",
    "ここでは小さなコーパスを使っている関係で、この結果は正直微妙なものです。それ\n",
    "では続いてPTB データセットという、より大きなコーパスを使って同じ実験を行っ\n",
    "てみます。まずはPTB データセットについて簡単に説明します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.4 PTBデータセット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまで私たちは、とても小さなテキストデータをコーパスとして使用してきまし\n",
    "た。ここでは、“本格的” なコーパス――それでいて大きすぎない手ごろなコーパス\n",
    "――を利用したいと思います。それはPenn Treebank（ペン・ツリー・バンク）と呼\n",
    "ばれるコーパスです（以降、PTB と略して表記します）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "私たちが利用するPTB コーパスは、word2vec の発明者であるTomas Mikolov\n",
    "氏のWeb ページで用意されているものです。このPTB コーパスはテキストファイ\n",
    "ルで提供されており、元となるPTB の文章に対して、いくつかの前処理が施されて\n",
    "います。どのような前処理かというと、たとえば、レアな単語を<unk>という特殊文\n",
    "字で置き換えたり（unk は「unknown」の略）、具体的な数字を「N」で置き換えたり\n",
    "といったことが行われています。そのような前処理を行った後のテキストデータを、\n",
    "私たちはPTB コーパスとして利用します。参考までに、図2-12 にPTB コーパス\n",
    "の中身を示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2_12](fig/2_12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図2-12 で示すように、PTB コーパスではひとつの文が1 行ごとに保存されてい\n",
    "ます。本書では、各文を連結したものを「ひとつの大きな時系列データ」として扱\n",
    "うことにします。またこのとき、各文の終わりに<eos>という特殊文字を挿入します\n",
    "（eos は「end of sentence」の略）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本書では、Penn Treebank のデータセットを簡単に利用できるように、専用\n",
    "のPython コードを準備しています。このファイルは、dataset/ptb.py にあり、\n",
    "「チャプターのディレクトリ（ch01、ch02、...）」から使うことを想定しています。\n",
    "たとえば、ch02 ディレクトリに移動してから、そのディレクトリ内において、\n",
    "「python show_ptb.py」のように利用します。それでは、ptb.py を使う例を次に\n",
    "示します（ ch02/show_ptb.py）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus size: 929589\n",
      "corpus[:30]: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "id_to_word[0]: aer\n",
      "id_to_word[1]: banknote\n",
      "id_to_word[2]: berlitz\n",
      "\n",
      "word_to_id['car']: 3856\n",
      "word_to_id['happy']: 4428\n",
      "word_to_id['lexus']: 7426\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from dataset import ptb\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "\n",
    "print('corpus size:', len(corpus))\n",
    "print('corpus[:30]:', corpus[:30])\n",
    "print()\n",
    "print('id_to_word[0]:', id_to_word[0])\n",
    "print('id_to_word[1]:', id_to_word[1])\n",
    "print('id_to_word[2]:', id_to_word[2])\n",
    "print()\n",
    "print(\"word_to_id['car']:\", word_to_id['car'])\n",
    "print(\"word_to_id['happy']:\", word_to_id['happy'])\n",
    "print(\"word_to_id['lexus']:\", word_to_id['lexus'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コーパスの扱い方については、これまでと同じです。corpus には単語ID のリス\n",
    "トが格納されます。id_to_word は単語ID から単語への変換を行うディクショナ\n",
    "リ、word_to_id は単語から単語ID への変換を行うディクショナリを表します。\n",
    "先のソースコードに示すように、ptb.load_data() でデータをロードします。こ\n",
    "のとき引数には、’train’、’test’、’valid’ のいずれかを指定します。これはそ\n",
    "れぞれ、「訓練用/ テスト用/ 検証用」のデータのいずれかに該当します。以上が\n",
    "ptb の使い方の説明です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.5 PTBデータセットでの評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PTB データセットに対してカウントベースの手法を適用してみましょう。ここで\n",
    "は、大きな行列にSVD を行うため、より高速なSVD を利用することを推奨します。\n",
    "それには、sklearn モジュールをインストールする必要があります。もちろん、シンプ\n",
    "ルなSVD（np.linalg.svd()）も使えますが、多くの時間とメモリが必要になりま\n",
    "す。それでは、ソースコードをまとめて示します（ ch02/count_method_big.py）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting  co-occurrence ...\n",
      "calculating PPMI ...\n",
      "1.0% done\n",
      "2.0% done\n",
      "3.0% done\n",
      "4.0% done\n",
      "5.0% done\n",
      "6.0% done\n",
      "7.0% done\n",
      "8.0% done\n",
      "9.0% done\n",
      "10.0% done\n",
      "11.0% done\n",
      "12.0% done\n",
      "13.0% done\n",
      "14.0% done\n",
      "15.0% done\n",
      "16.0% done\n",
      "17.0% done\n",
      "18.0% done\n",
      "19.0% done\n",
      "20.0% done\n",
      "21.0% done\n",
      "22.0% done\n",
      "23.0% done\n",
      "24.0% done\n",
      "25.0% done\n",
      "26.0% done\n",
      "27.0% done\n",
      "28.0% done\n",
      "29.0% done\n",
      "30.0% done\n",
      "31.0% done\n",
      "32.0% done\n",
      "33.0% done\n",
      "34.0% done\n",
      "35.0% done\n",
      "36.0% done\n",
      "37.0% done\n",
      "38.0% done\n",
      "39.0% done\n",
      "40.0% done\n",
      "41.0% done\n",
      "42.0% done\n",
      "43.0% done\n",
      "44.0% done\n",
      "45.0% done\n",
      "46.0% done\n",
      "47.0% done\n",
      "48.0% done\n",
      "49.0% done\n",
      "50.0% done\n",
      "51.0% done\n",
      "52.0% done\n",
      "53.0% done\n",
      "54.0% done\n",
      "55.0% done\n",
      "56.0% done\n",
      "57.0% done\n",
      "58.0% done\n",
      "59.0% done\n",
      "60.0% done\n",
      "61.0% done\n",
      "62.0% done\n",
      "63.0% done\n",
      "64.0% done\n",
      "65.0% done\n",
      "66.0% done\n",
      "67.0% done\n",
      "68.0% done\n",
      "69.0% done\n",
      "70.0% done\n",
      "71.0% done\n",
      "72.0% done\n",
      "73.0% done\n",
      "74.0% done\n",
      "75.0% done\n",
      "76.0% done\n",
      "77.0% done\n",
      "78.0% done\n",
      "79.0% done\n",
      "80.0% done\n",
      "81.0% done\n",
      "82.0% done\n",
      "83.0% done\n",
      "84.0% done\n",
      "85.0% done\n",
      "86.0% done\n",
      "87.0% done\n",
      "88.0% done\n",
      "89.0% done\n",
      "90.0% done\n",
      "91.0% done\n",
      "92.0% done\n",
      "93.0% done\n",
      "94.0% done\n",
      "95.0% done\n",
      "96.0% done\n",
      "97.0% done\n",
      "98.0% done\n",
      "99.0% done\n",
      "calculating SVD ...\n",
      "\n",
      "[query] you\n",
      " i: 0.6929551362991333\n",
      " we: 0.6785871386528015\n",
      " do: 0.5595189929008484\n",
      " someone: 0.5403239130973816\n",
      " 'd: 0.5268959403038025\n",
      "\n",
      "[query] year\n",
      " quarter: 0.6397143602371216\n",
      " next: 0.6298556923866272\n",
      " earlier: 0.5900095701217651\n",
      " last: 0.5836974382400513\n",
      " month: 0.5808842182159424\n",
      "\n",
      "[query] car\n",
      " auto: 0.6164522171020508\n",
      " luxury: 0.6029702425003052\n",
      " truck: 0.5526139736175537\n",
      " vehicle: 0.5468848347663879\n",
      " cars: 0.5350295305252075\n",
      "\n",
      "[query] toyota\n",
      " motor: 0.7372286915779114\n",
      " motors: 0.6691171526908875\n",
      " lexus: 0.6595461964607239\n",
      " honda: 0.6326931118965149\n",
      " nissan: 0.6092271208763123\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.util import most_similar, create_co_matrix, ppmi\n",
    "from dataset import ptb\n",
    "\n",
    "\n",
    "window_size = 2\n",
    "wordvec_size = 100\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "print('counting  co-occurrence ...')\n",
    "C = create_co_matrix(corpus, vocab_size, window_size)\n",
    "print('calculating PPMI ...')\n",
    "W = ppmi(C, verbose=True)\n",
    "\n",
    "print('calculating SVD ...')\n",
    "try:\n",
    "    # truncated SVD (fast!)\n",
    "    from sklearn.utils.extmath import randomized_svd\n",
    "    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5,\n",
    "                             random_state=None)\n",
    "except ImportError:\n",
    "    # SVD (slow)\n",
    "    U, S, V = np.linalg.svd(W)\n",
    "\n",
    "word_vecs = U[:, :wordvec_size]\n",
    "\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここではSVD を行うために、sklearn のrandomized_svd() というメソッドを\n",
    "利用します。これは、乱数を使ったTruncated SVD で、特異値の大きいものだけに\n",
    "限定して計算することで、通常のSVD よりも高速な計算が行えます。残りのコード\n",
    "は、前の小さなコーパスのときのコードとほとんど同じです。それでは、上のコード\n",
    "を実行してみましょう。そうすると、次の結果が得られます（Truncated SVD の場\n",
    "合は、乱数を使う関係で下記の結果は毎回異なります）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果を見ていくと、まずは「you」というクエリに対して、「i」や「we」の人称代\n",
    "名詞が上位を占めていることが分かります。これは文法的な使い方の点で共通の単語\n",
    "です。また、「year」がクエリの場合は「month」や「quarter」、「car」がクエリの場\n",
    "合は「auto」や「vehicle」など類義語が得られています。さらに、「toyota」をクエ\n",
    "リとした場合は、「nissan」や「honda」、「lexus」など自動車のメーカーやブランド\n",
    "が来ることも確認できます。このように、単語の意味的な点と文法的な点において、\n",
    "似た単語どうしが近いベクトルで表されました。これは私たちの感覚に近いものと言\n",
    "えそうです。\n",
    "おめでとうございます！ 私たちはついに「単語の意味」をベクトルにうまくエン\n",
    "コードすることに成功しました！ コーパスを使い、コンテキストの単語をカウント\n",
    "し、そしてそれをPPMI 行列に変換して、SVD による次元削減を行うことでより良\n",
    "い単語ベクトルを得られたのです。これこそが単語の分散表現であり、各単語は固定\n",
    "長のベクトルとして、そして密なベクトルとして表現されました。\n",
    "本章の実験では、一部の単語だけに対して類似単語を見たにすぎません。しかし、\n",
    "他の多くの単語でもそのような性質を確認できるでしょう。さらに大規模なコーパス\n",
    "を使用することで、より優れた単語の分散表現になることが期待されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 まとめ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章では、自然言語を対象として、特に「単語の意味」をコンピュータに理解させ\n",
    "ることをテーマに話を進めてきました。そのような目的を達成するために、シソーラ\n",
    "スを用いた手法を説明し、続いてカウントベースの手法を見てきました。\n",
    "シソーラスを用いる手法では、人の手によってひとつずつ単語の関連性を定義しま\n",
    "す。しかし、そのような作業はとても大変であり、また表現力の点で限界があります\n",
    "（細かなニュアンスを表せない、など）。一方、カウントベースの手法は、コーパスか\n",
    "ら自動的に単語の意味を抽出し、それをベクトルで表します。具体的には、単語の共\n",
    "起行列を作り、PPMI 行列に変換し、ロバスト性を高めるためにSVD による次元削\n",
    "減を行い、各単語の分散表現を得ました。そして、その分散表現は、意味的に（また\n",
    "文法的な使い方の点においても）似た単語がベクトル空間上で互いに近い場所にいる\n",
    "ことが確認できました。\n",
    "また本章では、コーパスのテキストデータを扱いやすくするための下準備の関数\n",
    "をいくつか実装しました。具体的には、ベクトル間の類似度を計測するための関数\n",
    "（cos_similarity()）や類似単語のランキング表示する関数（most_similar()）\n",
    "を実装しました。これらの関数は、次章以降でも使用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
