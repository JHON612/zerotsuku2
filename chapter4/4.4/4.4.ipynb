{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d7e535-4dc6-4217-bea3-f53090bb4970",
   "metadata": {},
   "source": [
    "# 4.4 word2vecに関する残りのテーマ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5178e474-f2a6-47d2-a943-61cdb2f80327",
   "metadata": {},
   "source": [
    "- word2vec の仕組みや実装については、ほとんど説明は終わりました。本節ではword2vec について、これまで扱いきれなかったテーマを紹介したいと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080399bf-658f-4581-8090-dae3fe6c74d2",
   "metadata": {},
   "source": [
    "## 4.4.1 word2vecを使ったアプリケーションの例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae174398-5e73-403a-9eaf-91c3c2576a08",
   "metadata": {},
   "source": [
    "- 大きなコーパス（WikipediaやGoogle News のテキストデータなど）で学習\n",
    "<br>↓単語の分散表現を活用\n",
    "- テキスト分類や文書クラスタリング、品詞タグ付け、感情分析などに活用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06773c9-2752-47b0-958d-1ba7d65ad891",
   "metadata": {},
   "source": [
    "- 自然言語をベクトルに変換できれば、一般的な機械学習の手法（ニューラルネットワークやSVMなど）が適用可能。 そのベクトルを別の機械学習システムの入力とすることができる\n",
    "- 単語の分散表現の学習と機械学習システムの学習は別のデータセットを使い、個別に学習を行うのが一般的\n",
    "    - 例\n",
    "        - 単語の分散表現については、Wikipedia のような汎用的なコーパスを使って学習\n",
    "        - 現在直面している問題に対して集められたデータを対象に、機械学習システム（SVM など）の学習\n",
    "        - ただし、現在直面する問題の学習データが大量に存在するのであれば、単語の分散表現と機械学習システムの学習を同時にゼロから行うことも考えられる\n",
    "![4_4](fig4/4_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fea4da-999e-4597-bfd2-2da74dbd051c",
   "metadata": {},
   "source": [
    "#### メールの自動分類システム"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6280bd00-cdf6-424d-b0a7-5be0c9d76b4d",
   "metadata": {},
   "source": [
    "- 前提\n",
    "    - 今あなたは、利用者が1 億人を超えるスマートフォンアプリの開発・運営をしているとします。\n",
    "    - 会社には、手に負えないほどのメールがユーザーから毎日届きます（または、Twitter などで多くのつぶやきを見つけることができます）。\n",
    "    - 好意的な意見がある一方で、中には、不満を持つユーザーの声も存在するでしょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4081578-88e6-4fe8-9c4d-c6b9c0fe0e95",
   "metadata": {},
   "source": [
    "- やりたいこと\n",
    "    - 送られてくるメール（そして“つぶやき” など）を自動で分類できるシステムを作れないかと考えます。\n",
    "    - たとえば、図4-22 のように、メールの内容からユーザーの感情を3段階に分類することはできないかと考えます。\n",
    "    - ユーザーの感情を正しく分類することができれば、不満を持つユーザーのメールから順に目を通すことができます。そうすれば、アプリの致命的な問題を発見し、早期に手を打つことができるかもしれません。\n",
    "![4_22](fig4/4_22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6065366b-1f9b-4c2e-af8f-f0c88d2d53fc",
   "metadata": {},
   "source": [
    "- 実現方法\n",
    "    - メールの自動分類システムを作成するには、まずはデータ（メール）を収集することから始めます。\n",
    "    - ここでの例ではユーザーから送られてきたメールを集め、そのメールに対して人手でラベル付けを行います。\n",
    "    - たとえば、3 段階の感情を表すラベル――positive / neutral / negative ――を付与します。\n",
    "    - そのラベル付けの作業が終わったら、学習済みのword2vec を用いてメールをベクトルへ変換します。\n",
    "    - 後は、感情分析を行う何らかの分類システム（SVM やニューラルネットワークなど）に対して、ベクトル化されたメールと感情ラベルを与えて学習させるのです。\n",
    "    - この例のように、自然言語を扱う問題は、単語の分散表現によってベクトルに変換することができます。それによって、通常の機械学習の手法で解くことができるようになります。さらに、word2vec の転移学習の恩恵が得られます。つまり、自然言語の多くのタスクでは、word2vec の単語の分散表現を利用することで精度の向上が期待できるのです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c3c254-fbf7-41b9-8bea-4b6c8f91ffd6",
   "metadata": {},
   "source": [
    "## 4.4.2 単語ベクトルの評価方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50416c5-3920-44df-8a6f-51a556b97d32",
   "metadata": {},
   "source": [
    "- 上記のアプリケーションにおける課題\n",
    "    - 単語の分散表現を作るシステム（word2vec）と、特定の問題に対して分類を行うシステム（たとえば感情を分類するSVM など）が別の場合、単語の分散表現の次元数が最終的な精度にどのように影響するかを調べるには、\n",
    "        - まず単語の分散表現の学習を行い、\n",
    "        - そしてその分散表現を使って、もうひとつの機械学習システムの学習が必要\n",
    "    - →つまり、2 段階の学習を行った上で評価する必要がある。\n",
    "\n",
    "        - さらにその場合、2 つのシステムにおいて最適なハイパーパラメータのためのチューニングも必要になり、多くの時間がかかってしまいます。\n",
    "\n",
    "- 解決法\n",
    "    - 単語の分散表現の良さを評価するにあたり、現実的なアプリケーションとは切り離して評価を行うというのが一般的によく行われます。\n",
    "    - その際によく用いられる評価指標が単語の「類似性」や「類推問題」による評価です。\n",
    "        - 単語の類似性の評価\n",
    "            - 人間が作成した単語類似度の評価セットを使って評価することが多く行われます。たとえば、0 から10 の間でスコア化するとして、「cat」と「animal」の類似度は8、「cat」と「car」の類似度は2 といったように、人が単語間の類似性を採点します）。そして、人が出したスコアとword2vec によるコサイン類似度のスコアを比較して、その相関性を見るのです。\n",
    "        - 類推問題による評価\n",
    "            - 「king : queen = man : ?」のような類推問題を出題し、その正解率でもって単語の分散表現の良さを測ります。たとえば、論文[27] では、類推問題による評価結果が掲載されています。ここでは、その結果から一部を抜粋して、図4-23 に示します。\n",
    "![4_23](fig4/4_23.png)\n",
    "- 図4-23 では、word2vec のモデル、単語の分散表現の次元数、そしてコーパスのサイズをパラメータとして、比較実験を行っています。\n",
    "    - 「Semantics」\n",
    "        - 単語の意味を類推する類推問題への正解率を示します。これはたとえば、「king : queen = actor : actress」のような単語の意味を問う問題です。\n",
    "    - 「Syntax」\n",
    "        - 単語の形態情報を問う問題で、たとえば「bad: worst = good : best」のような問題に相当します。\n",
    "\n",
    "        \n",
    "- 図4-23 の結果から、次のことが分かります。\n",
    "    - モデルによって精度が異なる（コーパスに応じて最適なモデルを選ぶ）\n",
    "    - コーパスが大きいほど良い結果になる（ビックデータは常に望まれる）\n",
    "    - 単語ベクトルの次元数は適度な大きさが必要（大きすぎても精度が悪くなる）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab876318-8fc8-4155-934b-18927824016b",
   "metadata": {},
   "source": [
    "# 4.5 まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b10a5b-1a9b-412c-bdb8-a96e46bf6538",
   "metadata": {},
   "source": [
    "- 本章では、word2vec の高速化をテーマに、前章のCBOWモデルに対して改良を加えました。\n",
    "- 具体的には、Embedding レイヤを実装しNegative Sampling という新しい手法を導入しました。この背景には、コーパスの語彙数が増えるのに比例し、計算量が増加することが挙げられます。\n",
    "\n",
    "- 本章での重要なテーマは、「すべて」ではなく「一部」を処理することです。結局のところ、人間がすべてを知ることができないように、コンピュータも（現在の性能では）すべてのデータを処理することは現実的ではありません。それよりも、自分にとって大切な少数のことに限定して取り組むほうが実りが多いのです。本章ではその考えに基づく手法―― Negative sampling――を詳しく見てきました。Negativesampling は、「すべて」の単語ではなく、「一部」の単語だけを対象にすることで、計算の効率化を達成しました。\n",
    "\n",
    "- 前章と本章にて、word2vec をテーマとする一連の話は終わりです。word2vec は自然言語の分野に多大な影響を与えてきました。そこで得られた単語の分散表現は、さまざまな自然言語処理のタスクに利用されています。さらにword2vec の思想は、自然言語だけではなく他の分野――音声、画像、動画など――にも応用されています。本章でword2vec をしっかりと理解したのであれば、その知識はさまざまな分野で役に立つはずです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b53a12a-a33f-476b-82bc-f3f4f12522f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
